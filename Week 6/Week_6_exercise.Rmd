---
title: "Week_6_exercise"
author: "Katz"
date: "2/22/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(broom)
library(car)
library(psych)
```


## For this exercise, you will be working with the student_happiness.csv file.

This file contains simulated data with several variables. The scenario is the following: 
You are interested in studying engineering student well being. One dimension of student health is their mental health. In particular, you have a measure of student happiness. This will be your outcome variable.

You have reason to believe that there are several factors that can contribute to a person's happiness. In particular, you believe that their class standing (e.g., undergrad, masters, phd), discipline (you only sampled mechanical, civil, and electrical engineering students), time spent outdoors, and time spent on zoom all play a role in a student's happiness. Sooo you go out and survey students, collecting each of these variables in addition to giving them a series of questions that let you calculate their happiness score. The composite score is what you now have in your `student_happiness.csv` file.

The objective here is to model the outcome as a function of the different predictors that you have. There will be a little less scaffolding than last week, but you can follow the class example in the Week_6_demo if you get stuck or can't think of what to do next...


First, it's probably a good idea to load in the data...
```{r}

pop_df <- read_csv("student_happiness.csv")

```


The `student_happiness.csv` file has an entire population of students in it. In reality, you will only be working with a sample of that total population. I have provided you with the full population so that you can test the effects of larger sample sizes on your model. I suggest you create a new dataframe that you create by sampling from the original dataframe with the `sample_n()` function from tidyverse. This sample is what you can use for the rest of your code. This will allow you to quickly change the sample size a few different times and look at how that affects model performance.


```{r}

sample_df <- pop_df %>% sample_n(size = 50) 

```
Start with a sample of 50 students and change it after you create your model to see what happens if you have a sample size of 100 or 200 instead of 50


Next, it's never a bad idea to start summarizing what you have. You can try visualizing things with geom_bar() or geom_point() (as appropriate) or make summary tables with things like group_by() and summarize() or count().

```{r}

summary(sample_df)

```

Now, it's probably time to start creating your model (or models...?).

Begin with just a simple model that has one predictor. You can choose which one.

```{r}
your_model <- lm(happiness ~ min_outdoors, data = sample_df)

summary(your_model)

```


Next, try creating a more complicated model to test how multiple predictors affect the model

```{r}
your_fancier_model <- lm(happiness ~ min_outdoors + min_zoom, data = sample_df)

summary(your_fancier_model)
```


Check your residuals for outliers and other model diagnostics. Try using the code in the book or in the `Week_6_demo.Rmd` file. You should try the Durbin Watson test for independent errors, looking at residuals, identifying potential influential cases, and multicollinearity (with the Variance Inflation Factor) or looking at a correlation matrix (be careful with this second option since you may have categorical predictors).

```{r}
sample_df$residuals <- resid(your_fancier_model) # notice how this is the same as looking at View(model_all) and then residuals or model_all$residuals
sample_df$residuals
sample_df$standardized.residuals <- rstandard(your_fancier_model)
sample_df$studentized.residuals <- rstudent(your_fancier_model)
sample_df$cooks.distance <- cooks.distance(your_fancier_model)
sample_df$dfbeta <- dfbeta(your_fancier_model)
sample_df$dffit <- dffits(your_fancier_model)
sample_df$leverage <- hatvalues(your_fancier_model)
sample_df$covariance.ratios <- covratio(your_fancier_model)
```

Plots for Residuals
```{r}
plot(your_fancier_model)
par(mfrow=c(1,1))
```
Durbin-Watson Test - tesing assumption of independence
This is close to 2 so good fit
```{r}
dwt(your_fancier_model)
```

VIF
Assessing the assumption fo Multicollinearity
```{r}
vif(your_fancier_model)
```

Try writing a few sentences in your markdown file about how you interpret the results of your model.

First modeled predicting student happiness with time outdoors.  The Adjusted R-squared value was 0.09396.  When added minutes on Zoom as a predictor variable the Adjusted R-squared value increased to 0.1789 .  I think that this means that 9.3936% of happiness can be explained by time outdoors and 8.494% can be explained by minutes on Zoom.

For the multiple model, the Durbin-Watson Test had a value of 2.129808, this is is close to 2 so the assumption of independence is good.

For the multiple model, the VIF range is 1.010454 to 1.010464


