---
title: "ProblemSet2"
author: "Amy Richardson"
date: "2/23/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

## Problem Set 1 - Quantitative Analysis

Working in:  
C:-Users-Amy Richardson-Dropbox-VT - PhD-ENGE 5714 - Quant-R Practice-ENGE5714-Problem Set 2

GitHub [link](https://github.com/AmyRichardsonVT/ENGE5714/blob/main/Problem%20Set%202/ProblemSet2.Rmd) to Code

```{r}
library(car)
library(psych)
library(ggplot2)
library(tidyverse)
library(broom)
```

***
# ------Part 1------

Create a correlation table between %2Y All, %4Y All, %4EngTot4yr, and %CSSTEMH4yr.  In text, report which correlations are statistically significant (if any) and comment on the strength of any relationships you observe. Run parametric and non-parametric versions. 


Read in Data and Create a DataFrame of  salient columns.
```{r}
all_degrees <- read.csv("PS_Degrees.csv", header = TRUE)

names(all_degrees)[1] <- "div_name"
percent_degrees <- all_degrees %>%
  select("Div_num", "X.2r", "X.4yr", "X.EngTot4yr", "X.CSSTEMH4yr" ) %>%
  transform(X.2r = X.2r * 100) %>%
  transform(X.4yr = X.4yr * 100) %>%
  transform(X.EngTot4yr = X.EngTot4yr * 100) %>%
  transform(X.CSSTEMH4yr = X.CSSTEMH4yr * 100)
```

* Define variables:
     + X.2r = Percent of total degrees awarded from 2-yr public VA institutions, 2012-13 through 2016-17
     + X.4yr = Percent of total degrees awarded from 4-yr institutions, 2012-13 through 2016-17
     + X.EngTot4yr = Percent of total degrees from 4-yr institutions during five-year time period that were Engineering degrees
     + X.CSSTEMH4yr = Percent of all STEMH degrees awarded from 4-yr institutions that were CS/IS during five-year time period

Remove rows with NA since the entire row contains NA values
```{r}
percent_degrees <- percent_degrees[-c(135:144), ]
```

Check to see if the data is normally distributed.  First Plot this data.
```{r}
degree_df_long <- percent_degrees %>% 
  pivot_longer(cols = X.2r:X.CSSTEMH4yr, names_to = "Variable", values_to = "Percentages")

degree_df_long %>% 
  ggplot(aes(x = Percentages, fill = Variable)) +
  geom_histogram() +
  facet_wrap(Variable ~., scales = "free") +
  theme(legend.position = "none")



```

Run Q-QPlots
```{r}
degree_df_long %>% 
  ggplot(aes(sample=Percentages)) +
  stat_qq() +
  stat_qq_line() +
  facet_wrap(Variable ~ ., scales = "free")
```

Run correlation 
```{r}
my_correlations <- percent_degrees %>% select(X.2r, X.4yr, X.EngTot4yr, X.CSSTEMH4yr) %>% cor()
print(my_correlations)
```

Run Spearman's Corelation
```{r}
my_spearman_correlations <- percent_degrees %>% select(X.2r, X.4yr, X.EngTot4yr, X.CSSTEMH4yr) %>% cor(method="spearman")
print(my_spearman_correlations)
```

Get p-values
```{r}
cor.test(percent_degrees$X.2r, percent_degrees$X.EngTot4yr)
```
```{r}
cor.test(percent_degrees$X.2r, percent_degrees$X.CSSTEMH4yr)
```
```{r}
cor.test(percent_degrees$X.4yr, percent_degrees$X.EngTot4yr)
```
```{r}
cor.test(percent_degrees$X.4yr, percent_degrees$X.CSSTEMH4yr)
```

### Report on Results
*report which correlations are statistically significant (if any) and comment on the strength of any relationships you observe*
Pearson - Use if you have:

Large (> 30) sample size
Normally distributed data
Interval data
Values for r  lie in [-1,1]

Rules of thumb for interpretation:

very small effect size: 0 < ∣r∣ < 0.1
small effect size: 0.1 < ∣r∣ < 0.3
medium effect size: 0.3 < ∣r∣ < 0.5
large effect size: 0.5 < ∣r∣ < 1



***
# ------Part 2------
Run a simple regression using principal salary in a county to predict teacher salary in that county. Interpret the results of the model and discuss how good the fit is. Discuss in practical terms how to interpret the regression coefficient. Evaluate if there are any outliers and adjust the model if needed.

Read in Data and Create a DataFrame of  salient columns.
Note that in the original data set the following sections were removed:
     + Governor's Schools
     + Special Education Regoinal Programs
     + Career and Technical (Vocational) Education Regional Programs
     + Regional Alternative Education Programs
     
```{r warning=FALSE}
prinData <- read.csv("Prin_Salary.csv", header = TRUE)
teachData <- read.csv("Teach_Salary.csv", header = TRUE)

```

Clean both DFs so that we have only the columns we need.  We will only look at 2005 and 2016
```{r}
prinData_sub <- prinData %>%
  select(div_name, FY2005P, FY2016P) 
  
teachData_sub <- teachData %>%
  select(div_name, FY2005T, FY2016T)
```

Need to change columns containing salary to integers
```{r}
prinData <- prinData%>%
    mutate_at(vars(starts_with("FY20")), as.numeric)
```

Join Principle & Teacher data with full join.
```{r}
salary_data <- inner_join(prinData_sub,teachData_sub)

```
Some columns have salaries < 0, replace them with NA
```{r}
salary_data [salary_data  <= 0] <- NA
```

Create Plots for 2005 and 2016
```{r}
salary_data %>% 
  ggplot(aes(x = FY2005P, y = FY2005T)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(title = "2005",  x = "Principal Salaries", y = "Teacher Salaries")
 
salary_data %>% 
  ggplot(aes(x = FY2016P, y = FY2016T)) +
  geom_point() +
  geom_smooth(method = "lm")+
  labs(title = "2016", x = "Principal Salaries", y = "Teacher Salaries")

```

Create Model for 2005
```{r}
fit_salary_2005 <- lm(FY2005T ~ FY2005P, data = salary_data)
summary(fit_salary_2005)
```
Create Model for 2016
```{r}
fit_salary_2016 <- lm(FY2016T ~ FY2016P, data = salary_data)
summary(fit_salary_2016)
```

Look for Outliers and/or Influential Cases 2005
```{r}
salary_data$residual05 <- resid(fit_salary_2005)
salary_data$standardized.residuals05 <- rstandard(fit_salary_2005)
salary_data$studentized.residuals05 <- rstudent(fit_salary_2005)
salary_data$cooks.distance05 <-cooks.distance(fit_salary_2005)
salary_data$dfbeta05 <- dfbeta(fit_salary_2005)
salary_data$dffit05 <- dffits(fit_salary_2005)
salary_data$leverage05 <- hatvalues(fit_salary_2005)
salary_data$covariance.ratios05 <- covratio(fit_salary_2005)

fit_salary_df <- salary_data %>% 
  mutate(large.residual05 = case_when(standardized.residuals05 > 2 | standardized.residuals05 < -2 ~ TRUE,
                                    abs(standardized.residuals05) <= 2 ~ FALSE))

fit_salary_df %>% filter(large.residual05 == TRUE) %>% head()


plot(fit_salary_2005)
```
There is an indication that there are no outliers or influential cases since there are no residuals greater less than -2 or greater than 2.  This is confirmed by the dataframe and the plots.


Look for Outliers and/or Influential Cases 2016
```{r}
salary_data$residual16 <- resid(fit_salary_2016)
salary_data$standardized.residuals16 <- rstandard(fit_salary_2016)
salary_data$studentized.residuals16 <- rstudent(fit_salary_2016)
salary_data$cooks.distance16 <-cooks.distance(fit_salary_2016)
salary_data$dfbeta16 <- dfbeta(fit_salary_2016)
salary_data$dffit16 <- dffits(fit_salary_2016)
salary_data$leverage16 <- hatvalues(fit_salary_2016)
salary_data$covariance.ratios16 <- covratio(fit_salary_2016)

fit_salary_df <- salary_data %>% 
  mutate(large.residual16 = case_when(standardized.residuals16 > 2 | standardized.residuals16 < -2 ~ TRUE,
                                    abs(standardized.residuals16) <= 2 ~ FALSE))

fit_salary_df %>% filter(large.residual16 == TRUE) %>% head()

plot(fit_salary_2016)
```


There is an indication that there are no outliers or influential cases since there are no residuals greater less than -2 or greater than 2.  This is confirmed by the dataframe and the plots.

### Report on Results
*Discuss in practical terms how to interpret the regression coefficient*
xxxxxxx

***
# ------Part 3------
Randomly divide the group of counties from (2) in two halves.  Run a simple regression on each half.  Compare the results from each of these two samples with what you know the population answers to be and discuss any differences you see in these two estimates of the true values and if the confidence intervals from these samples encompass the results of the original population regression from (2). Plot scatterplots of these different datasets with models and use ggplot to add the lines on too.  


Let's take our joined dataframe (salary_data) and split the 130 observations into two ramdom groups.  
```{r}
set.seed(37645)                           
dummy_sep <- rbinom(nrow(salary_data), 1, 0.5)  

salary_sample1 <- salary_data[dummy_sep == 0, ]
salary_sample2 <- salary_data[dummy_sep == 1, ]
               
```

Create a Linear Model on each set for 2005
```{r}
fit_salary1_2005 <- lm(FY2005T ~ FY2005P, data = salary_sample1)
summary(fit_salary1_2005)
```
```{r}
fit_salary2_2005 <- lm(FY2005T ~ FY2005P, data = salary_sample2)
summary(fit_salary2_2005)
```
Create Plots for 2005
```{r}
salary_sample1 %>% 
  ggplot(aes(x = FY2005P, y = FY2005T)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(title = "2005:  Sample Group 1",  x = "Principal Salaries", y = "Teacher Salaries")
 
salary_sample2 %>% 
  ggplot(aes(x = FY2005P, y = FY2005T)) +
  geom_point() +
  geom_smooth(method = "lm")+
  labs(title = "2005:  Sample Group 2", x = "Principal Salaries", y = "Teacher Salaries")
```

### Report on Results
*discuss any differences you see in these two estimates of the true values and if the confidence intervals from these samples encompass the results of the original population regression from *
xxxxxxx

***
# ------Part 4------
Run a multiple regression.  Choose an outcome variable of interest from the one of the degree percentage variables.  Choose one predictor variable from the tables provided to pretend it is the theory variable. Choose two additional predictor variables from the tables to be the “additional” possible predictor variables. Interpret the results of the models and discuss how good a fit are the models, including whether or not the second level model is better than the first. Discuss in practical terms how to interpret the regression coefficient. Evaluate if there are any outliers and adjust the model if needed.  Compute relevant diagnostics, produce and interpret plots, etc. to demonstrate your knowledge of various means of checking assumptions.

I will use Percentage of all STEM-H degree as the outcome variable all for 2015.


Read in the new data, select the relevant columns, ensure the data are numbers, and join data.
```{r}
VA_chars <- read.csv("VA_characteristics.csv", header = TRUE)

VA_chars_sub <- VA_chars %>%
  select(Locality, HS2015, Unemploy2015, Comp_index_2014_2016) 

degree_sub <- all_degrees %>%
  select(div_name, X.STEMHall) 

names(VA_chars_sub)[1] <- "div_name"

VA_chars_sub <- VA_chars_sub %>%
  mutate_at(vars(starts_with ("HS")), as.numeric) 

VA_chars_sub <- VA_chars_sub %>%
  mutate_at(vars(starts_with ("Une")), as.numeric) 

VA_chars_sub <- VA_chars_sub %>%
  mutate_at(vars(starts_with ("Comp")), as.numeric)

char_degree <- inner_join(VA_chars_sub, degree_sub, by = "div_name")

```

NEED TO PICK NEW VARIABLES.....:)
Using the Hierarchical method I am going to start using unemployment rates to predict % of STEM-H degrees at all institutions of higher ed.
```{r}
fit_single_degree <- lm(X.STEMHall ~ Comp_index_2014_2016, data = char_degree)
summary(fit_single_degree)

char_degree %>% 
  ggplot(aes(x = Unemploy2015, y = X.STEMHall)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(title = "Single Regression 2015",  x = "Unemployment Rate", y = "Percentage of STEM-H Degrees")
```









### Report on Results
*Interpret the results of the models and discuss how good a fit are the models, including whether or not the second level model is better than the first. Discuss in practical terms how to interpret the regression coefficient. Evaluate if there are any outliers and adjust the model if needed.  Compute relevant diagnostics, produce and interpret plots, etc. to demonstrate your knowledge of various means of checking assumptions.*
xxxxxxx
